#!/bin/bash

#PBS -q condo05
#PBS -N GATK_pipeline_6populations
#PBS -l select=1:ncpus=64:mem=502gb
#PBS -l walltime=168:00:00
#PBS -W group_list=x-ccast-prj-hulke

# Pipeline for calling variants on many samples with GATK, using parallelization.
# Authors: Kyle Keepers, Brian Smart
# Date: 6/26/2024

# Enable exit on error and proper error propagation in pipes
set -e
set -o pipefail

# Function for logging
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a pipeline.log
}

# Function for error handling
handle_error() {
    log "Error occurred in $1 at line $2"
    exit 1
}

# Set up error handling
trap 'handle_error "${BASH_SOURCE}" "${LINENO}"' ERR

# Checkpointing function
checkpoint() {
    touch "CHECKPOINT_$1"
    log "Checkpoint $1 reached"
}

# Function to check if a checkpoint exists
check_checkpoint() {
    if [ -f "CHECKPOINT_$1" ]; then
        log "Resuming from checkpoint $1"
        return 0
    else
        return 1
    fi
}

# Change to the working directory
cd /mmfs1/projects/brent.hulke/HaplotypeStructureForGenomicSelection/
log "Starting pipeline execution"

# Set the number of threads to use
threads=53

# Load required modules
module load parallel
module load samtools
module load bcftools
module load gatk/4.1.8.1-gcc-2onk

# Set path to BEAGLE jar file
BEAGLE_JAR="/mmfs1/projects/brent.hulke/Software/beagle.01Mar24.d36.jar"

# Set paths to reference files
pathToBED=/mmfs1/home/brian.smart/projects/sunflower_reference_genomes/HA412HOv2_w_CPMT/Ha412HOv2_w_CPMT_split53.fa.bed
pathToReference=/mmfs1/home/brian.smart/projects/sunflower_reference_genomes/HA412HOv2_w_CPMT/Ha412HOv2_w_CPMT.fa
sampleMap=6populations.sample_map

# Define population sample files
population_files=(
    "RHA474_R101dmr_6_RHA344_NIDHO__NS-H-924_4_RHA418_RHA419_3_RO12-13__RHA274_PRS5_5_RHA475_RHA464_sampleNames.txt"
    "RHA485_4_RHA473_3_RHA462__PET-1gammarays_6116_sampleNames.txt"
    "RHA476_687_sampleNames.txt"
    "RHA476_IS3401_sampleNames.txt"
    "Nectar_sampleNames.txt"
    "Pericarp_sampleNames.txt"
)

log "Configuration complete"

# Check for key files
if [ ! -f "$sampleMap" ]; then
    log "Error: Sample map file not found: $sampleMap"
    exit 1
fi

if [ ! -f "$pathToBED" ]; then
    log "Error: BED file not found: $pathToBED"
    exit 1
fi

if [ ! -f "$pathToReference" ]; then
    log "Error: Reference file not found: $pathToReference"
    exit 1
fi

########################################################################################
############################Step 3: GenomicsDBImport####################################
########################################################################################
if ! check_checkpoint "GENOMICSDBIMPORT"; then
    log "Starting GenomicsDBImport step"
    
    # Create a list of chromosomes from the BED file
    awk '{print $1}' $pathToBED | uniq > chr.list

    # Run GenomicsDBImport to create a genomics database
    #gatk --java-options "-Xmx400G -Xms400G" GenomicsDBImport \
    # --genomicsdb-workspace-path OutputFromGenomicsDBImport \
    # --intervals chr.list \
    # --sample-name-map $sampleMap \
    # --reader-threads $threads \
    # --overwrite-existing-genomicsdb-workspace true \
    # --batch-size 50

    checkpoint "GENOMICSDBIMPORT"
else
    log "Skipping GenomicsDBImport step (already completed)"
fi

########################################################################################
#############################Step 4: Combine GVCFs######################################
########################################################################################
if ! check_checkpoint "COMBINE_GVCFS"; then
    log "Starting Combine GVCFs step"

    # Function to combine VCFs for a specific region
    #function COMBINEVCFS_AND_REFORMAT {
    #    REGION=$1
    #    REF=$2
    #    gatk --java-options "-Xmx8G -Xms6G" GenotypeGVCFs \
    #    --intervals $REGION \
    #    --reference $REF \
    #    -V gendb://OutputFromGenomicsDBImport \
    #    -O Combined_${REGION}.vcf.gz
    #}
    #export -f COMBINEVCFS_AND_REFORMAT

    # Process each region in parallel
    #cat $pathToBED | awk '{print $1":"$2"-"$3}' | parallel --will-cite -j $threads "COMBINEVCFS_AND_REFORMAT {} $pathToReference"

    # Create a list of combined VCF files
    #awk '{print "Combined_"$1":"$2"-"$3".vcf.gz"}' $pathToBED > Regions

    # Concatenate all combined VCFs
    #bcftools concat --file-list Regions -O z > Combined_6populations.vcf.gz

    # Reformat missing data for BEAGLE imputation
    #zcat Combined_6populations.vcf.gz | \
    #sed 's/\t\.:\./\t\.|\.:\./g' | \
    #sed 's/\t\.:0,0,0/\t\.|\.:0,0,0/g' | \
    #sed 's/\t\.:0,0,1/\t\.|\.:0,0,0/g' | \
    #sed 's/\t\.:1,0,0/\t\.|\.:0,0,0/g' | \
    #bgzip > Combined_for_imputation_6populations.vcf.gz

    # Index the combined and reformatted VCF
    #tabix -p vcf Combined_for_imputation_6populations.vcf.gz

    checkpoint "COMBINE_GVCFS"
else
    log "Skipping Combine GVCFs step (already completed)"
fi

########################################################################################
#############################Step 5: Split VCF by Population############################
########################################################################################
if ! check_checkpoint "SPLIT_VCF"; then
    log "Starting Split VCF by Population step"

    # Split VCF by population and index each split VCF
    for pop_file in "${population_files[@]}"; do
        pop_name=$(basename "$pop_file" _sampleNames.txt)
        bcftools view -S "$pop_file" -Oz -o "${pop_name}_split.vcf.gz" Combined_for_imputation_6populations.vcf.gz
        tabix -p vcf "${pop_name}_split.vcf.gz"
    done

    checkpoint "SPLIT_VCF"
else
    log "Skipping Split VCF by Population step (already completed)"
fi

########################################################################################
#############################Step 6: BEAGLE Imputation##################################
########################################################################################
if ! check_checkpoint "BEAGLE_IMPUTATION"; then
    log "Starting BEAGLE Imputation step"

    # Run BEAGLE imputation for each population
    for pop_file in "${population_files[@]}"; do
        pop_name=$(basename "$pop_file" _sampleNames.txt)
        java -Xmx400G -Xms400G -jar ${BEAGLE_JAR} gt="${pop_name}_split.vcf.gz" nthreads=64 out="${pop_name}_Imputed"
    done

    checkpoint "BEAGLE_IMPUTATION"
else
    log "Skipping BEAGLE Imputation step (already completed)"
fi

########################################################################################
#############################Step 7: Filter for Biallelic SNPs##########################
########################################################################################
if ! check_checkpoint "FILTER_BIALLELIC_SNPS"; then
    log "Starting Filter for Biallelic SNPs step"

    # Filter for biallelic SNPs for each population
    for pop_file in "${population_files[@]}"; do
        pop_name=$(basename "$pop_file" _sampleNames.txt)
        bcftools view --max-alleles 2 --exclude-types indels "${pop_name}_Imputed.vcf.gz" --threads 64 -O z -o "${pop_name}_Imputed_biallelicSNPs.vcf.gz"
    done

    checkpoint "FILTER_BIALLELIC_SNPS"
else
    log "Skipping Filter for Biallelic SNPs step (already completed)"
fi

########################################################################################
#############################Step 8: Cleanup############################################
########################################################################################
log "Starting Cleanup step"

# Remove intermediate files
rm -f Combined_Ha* Combined_NC* Regions chr.list

# Remove split VCFs if no longer needed
#for pop_file in "${population_files[@]}"; do
#    pop_name=$(basename "$pop_file" _sampleNames.txt)
#    rm -f "${pop_name}_split.vcf.gz" "${pop_name}_split.vcf.gz.tbi"
#done

# NOTE: Uncomment the following line if you want to remove the combined VCF files
# rm -f Combined_6populations.vcf.gz Combined_for_imputation_6populations.vcf.gz*
log "Pipeline execution completed successfully"
